\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{ngerman}
\usepackage{enumerate}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{hyperref}
 
\title{MineCrawler}
\subtitle{WebCrawler}
\author{
  Tabea Treutwein\\
  \texttt{tabea.treutwein@st.ovgu.de}\\
  \and
  Dennis Meyer\\
  \texttt{dennis.meyer@st.ovgu.de}\\
  \and
  Alexander Simmer\\
  \texttt{asimmer@st.ovgu.de}\\
  \and
  Kilian GÃ¤rtner\\
  \texttt{kilian.gaertner@st.ovgu.de}\\
}

\date{\today}
 
\begin{document}
\maketitle
 
\section{Task}
The main task of this project was to index websites beginning on an initial website, the seed. After indexing the website, an query is performed to retrieve the best matching websites for the query. Like we did in the first task, we have used Apache Lucene for indexing the websites, the HTML parsing is done by jsoup library.

\section{Usage}

The Java executable is a console based program, you have to run it from the console with
\begin{verbatim}
	java -jar IR13_Assigment_2.1_MineCrawler.jar
\end{verbatim}
The Java executable accepts the following arguments:

\begin{description}
  \item[-u URL] \hfill \\
  The initial website to start the crawling
  \item[-c] \hfill \\
  Should the result list printed on the console instead write in a file(will be created by default)
  \item[-d NUMBER] \hfill \\
  The maximum recursive depth of the crawling. Default is 5. 
\end{description}
Note that the order of the arguments does not matter. In case none of these arguments are specified, the user will be prompted to enter additional parameters (i.e. number of results, search query). Those are quite self-explanatory and will therefore not be elaborated here.

\subsection{Query format}
The query format is defined by the used Library Apache Lucene and can be found \href{http://lucene.apache.org/core/4_6_0/queryparser/org/apache/lucene/queryparser/classic/package-summary.html#package_description}{on this website}.  \\
The possible search fields are 
\begin{itemize}
	\item title
	\item body
\end{itemize}

\section{Process of Solution}
The program asks the user for the neccessary information to execute the task via arguments from command line or via a little wizard.
With the seed url and the deepth of the crawling the cache manager will be asked, wether he has a cache file for this or not. Using a cache with a maximum hold of an hour increases the performance dramatically, because the bottleneck of this, the crawling, is reduced to very few ones. When the user executes a query on the same seed and the same deepth, the cache will be used instead of always crawling websites, which takes a massive amount of time depening on the network and the web server.
After crawling websites via deep first search or using cache the normal search with Lucene is done and the user gets the results.

\section{Libraries}
The used libraries for this project are

\begin{enumerate}
	\item \href{http://lucene.apache.org/}{Apache Lucene} - Creating an index and executing the query
	\item \href{http://www.joda.org/joda-time/}{JodaTime} - For easier date handeling(normal java implementation isn't the best...)
	\item \href{http://commons.apache.org/proper/commons-cli/}{Apache Commons CLI} - Easy parsing of console arguments in POSIX style
	\item \href{http://jsoup.org/}{jsoup} - Parsing html and provide simple access for the title and the body

\end{enumerate}

\end{document}